"""
æµå¤±é¢„æµ‹æ¨¡å—
ä½¿ç”¨æœºå™¨å­¦ä¹ é¢„æµ‹ç”¨æˆ·æµå¤±æ¦‚ç‡
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (accuracy_score, classification_report, 
                             confusion_matrix, roc_curve, auc)
import joblib

plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False


class ChurnPredictor:
    """ç”¨æˆ·æµå¤±é¢„æµ‹å™¨"""
    
    def __init__(self, rfm_df):
        """
        åˆå§‹åŒ–é¢„æµ‹å™¨
        
        Args:
            rfm_df: åŒ…å« RFM æ•°æ®çš„ DataFrame
        """
        self.rfm = rfm_df.copy()
        self.model = None
        self.X_train, self.X_test, self.y_train, self.y_test = None, None, None, None
        self.predictions = None
        self.probabilities = None
        
    def prepare_data(self, churn_threshold=60, test_size=0.3):
        """
        å‡†å¤‡è®­ç»ƒå’Œæµ‹è¯•æ•°æ®
        
        Args:
            churn_threshold: å®šä¹‰æµå¤±çš„ Recency é˜ˆå€¼ï¼ˆå¤©ï¼‰
            test_size: æµ‹è¯•é›†æ¯”ä¾‹
        """
        print(f"ğŸ“Š å‡†å¤‡æ•°æ®...")
        print(f"æµå¤±å®šä¹‰: Recency > {churn_threshold} å¤©")
        
        # åˆ›å»ºæµå¤±æ ‡ç­¾
        self.rfm['Churn'] = (self.rfm['Recency'] > churn_threshold).astype(int)
        
        # ç‰¹å¾é€‰æ‹©
        features = ['Recency', 'Frequency', 'Monetary']
        X = self.rfm[features]
        y = self.rfm['Churn']
        
        print(f"æ€»æ ·æœ¬æ•°: {len(X)}")
        print(f"æµå¤±ç”¨æˆ·: {y.sum()} ({y.mean()*100:.1f}%)")
        print(f"æ´»è·ƒç”¨æˆ·: {(y==0).sum()} ({(1-y.mean())*100:.1f}%)")
        
        # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
            X, y, test_size=test_size, random_state=42, stratify=y
        )
        
        print(f"è®­ç»ƒé›†: {len(self.X_train)} æ ·æœ¬")
        print(f"æµ‹è¯•é›†: {len(self.X_test)} æ ·æœ¬")
        
        return self
    
    def train_logistic_regression(self):
        """è®­ç»ƒé€»è¾‘å›å½’æ¨¡å‹"""
        print("\nğŸ¤– è®­ç»ƒé€»è¾‘å›å½’æ¨¡å‹...")
        
        self.model = LogisticRegression(random_state=42, max_iter=1000)
        self.model.fit(self.X_train, self.y_train)
        
        # é¢„æµ‹
        self.predictions = self.model.predict(self.X_test)
        self.probabilities = self.model.predict_proba(self.X_test)[:, 1]
        
        # è¯„ä¼°
        accuracy = accuracy_score(self.y_test, self.predictions)
        print(f"æ¨¡å‹å‡†ç¡®ç‡: {accuracy:.4f}")
        
        return self
    
    def train_random_forest(self):
        """è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹ï¼ˆå¯¹æ¯”ç”¨ï¼‰"""
        print("\nğŸŒ² è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹...")
        
        rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
        rf_model.fit(self.X_train, self.y_train)
        
        rf_predictions = rf_model.predict(self.X_test)
        rf_accuracy = accuracy_score(self.y_test, rf_predictions)
        
        print(f"éšæœºæ£®æ—å‡†ç¡®ç‡: {rf_accuracy:.4f}")
        
        # ç‰¹å¾é‡è¦æ€§
        importances = pd.DataFrame({
            'feature': ['Recency', 'Frequency', 'Monetary'],
            'importance': rf_model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        print("\nç‰¹å¾é‡è¦æ€§:")
        print(importances)
        
        return rf_model
    
    def evaluate_model(self):
        """è¯¦ç»†è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        print("\nğŸ“ˆ æ¨¡å‹è¯„ä¼°æŠ¥å‘Š:")
        print("="*50)
        print(classification_report(self.y_test, self.predictions, 
                                   target_names=['æ´»è·ƒ', 'æµå¤±']))
        
        # æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(self.y_test, self.predictions)
        print("\næ··æ·†çŸ©é˜µ:")
        print(cm)
        
        # è®¡ç®—å„é¡¹æŒ‡æ ‡
        tn, fp, fn, tp = cm.ravel()
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
        
        print(f"\nç²¾ç¡®ç‡ (Precision): {precision:.4f}")
        print(f"å¬å›ç‡ (Recall): {recall:.4f}")
        print(f"ç‰¹å¼‚åº¦ (Specificity): {specificity:.4f}")
        
        return {
            'precision': precision,
            'recall': recall,
            'specificity': specificity,
            'confusion_matrix': cm
        }
    
    def plot_confusion_matrix(self, save_path='../data/confusion_matrix.png'):
        """ç»˜åˆ¶æ··æ·†çŸ©é˜µçƒ­åŠ›å›¾"""
        cm = confusion_matrix(self.y_test, self.predictions)
        
        fig, ax = plt.subplots(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,
                   xticklabels=['æ´»è·ƒ', 'æµå¤±'],
                   yticklabels=['æ´»è·ƒ', 'æµå¤±'])
        
        ax.set_title('æµå¤±é¢„æµ‹ - æ··æ·†çŸ©é˜µ', fontsize=16, fontweight='bold', pad=20)
        ax.set_xlabel('é¢„æµ‹æ ‡ç­¾', fontsize=12)
        ax.set_ylabel('çœŸå®æ ‡ç­¾', fontsize=12)
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"âœ… æ··æ·†çŸ©é˜µå·²ä¿å­˜: {save_path}")
        plt.close()
    
    def plot_roc_curve(self, save_path='../data/roc_curve.png'):
        """ç»˜åˆ¶ ROC æ›²çº¿"""
        fpr, tpr, _ = roc_curve(self.y_test, self.probabilities)
        roc_auc = auc(fpr, tpr)
        
        fig, ax = plt.subplots(figsize=(8, 6))
        ax.plot(fpr, tpr, color='darkorange', lw=2, 
                label=f'ROC æ›²çº¿ (AUC = {roc_auc:.3f})')
        ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', 
                label='éšæœºåˆ†ç±»å™¨')
        
        ax.set_xlim([0.0, 1.0])
        ax.set_ylim([0.0, 1.05])
        ax.set_xlabel('å‡é˜³æ€§ç‡ (False Positive Rate)', fontsize=12)
        ax.set_ylabel('çœŸé˜³æ€§ç‡ (True Positive Rate)', fontsize=12)
        ax.set_title('æµå¤±é¢„æµ‹ - ROC æ›²çº¿', fontsize=16, fontweight='bold', pad=20)
        ax.legend(loc='lower right')
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"âœ… ROC æ›²çº¿å·²ä¿å­˜: {save_path}")
        plt.close()
        
        return roc_auc
    
    def plot_feature_importance(self, save_path='../data/feature_importance.png'):
        """ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§ï¼ˆé€»è¾‘å›å½’ç³»æ•°ï¼‰"""
        # å¯¹äºé€»è¾‘å›å½’ï¼Œç³»æ•°ä»£è¡¨ç‰¹å¾é‡è¦æ€§
        coefficients = self.model.coef_[0]
        features = ['Recency', 'Frequency', 'Monetary']
        
        # å½’ä¸€åŒ–ç³»æ•°åˆ° 0-1 èŒƒå›´
        importance = np.abs(coefficients)
        importance = importance / importance.sum()
        
        fig, ax = plt.subplots(figsize=(8, 6))
        colors = ['#e74c3c', '#3498db', '#2ecc71']
        bars = ax.bar(features, importance, color=colors, edgecolor='black')
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, imp in zip(bars, importance):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{imp:.3f}',
                   ha='center', va='bottom', fontsize=11, fontweight='bold')
        
        ax.set_title('ç‰¹å¾é‡è¦æ€§åˆ†æ', fontsize=16, fontweight='bold', pad=20)
        ax.set_xlabel('ç‰¹å¾', fontsize=12)
        ax.set_ylabel('é‡è¦æ€§æƒé‡', fontsize=12)
        ax.set_ylim(0, max(importance) * 1.2)
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"âœ… ç‰¹å¾é‡è¦æ€§å›¾å·²ä¿å­˜: {save_path}")
        plt.close()
    
    def predict_churn_risk(self, output_path='../data/churn_predictions.csv'):
        """
        ä¸ºæ‰€æœ‰ç”¨æˆ·é¢„æµ‹æµå¤±é£é™©
        
        Returns:
            DataFrame åŒ…å«é¢„æµ‹ç»“æœ
        """
        # ä¸ºæ‰€æœ‰ç”¨æˆ·é¢„æµ‹
        all_features = self.rfm[['Recency', 'Frequency', 'Monetary']]
        self.rfm['Churn_Probability'] = self.model.predict_proba(all_features)[:, 1]
        self.rfm['Churn_Risk_Level'] = pd.cut(
            self.rfm['Churn_Probability'],
            bins=[0, 0.3, 0.7, 1.0],
            labels=['ä½é£é™©', 'ä¸­é£é™©', 'é«˜é£é™©']
        )
        
        # ä¿å­˜é¢„æµ‹ç»“æœ
        self.rfm.to_csv(output_path)
        print(f"\nâœ… æµå¤±é¢„æµ‹ç»“æœå·²ä¿å­˜: {output_path}")
        
        # ç»Ÿè®¡é£é™©åˆ†å¸ƒ
        risk_dist = self.rfm['Churn_Risk_Level'].value_counts()
        print("\næµå¤±é£é™©åˆ†å¸ƒ:")
        for level, count in risk_dist.items():
            pct = count / len(self.rfm) * 100
            print(f"  {level}: {count} äºº ({pct:.1f}%)")
        
        return self.rfm
    
    def save_model(self, path='../data/churn_model.pkl'):
        """ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹"""
        joblib.dump(self.model, path)
        print(f"\nâœ… æ¨¡å‹å·²ä¿å­˜: {path}")


def main():
    """ä¸»å‡½æ•°"""
    import os
    
    # åŠ è½½ RFM æ•°æ®
    print("="*60)
    print("ğŸš€ ç”¨æˆ·æµå¤±é¢„æµ‹åˆ†æ")
    print("="*60)
    
    rfm_path = '../data/rfm_results.csv'
    if not os.path.exists(rfm_path):
        print(f"é”™è¯¯: æ‰¾ä¸åˆ° RFM æ•°æ®æ–‡ä»¶ {rfm_path}")
        print("è¯·å…ˆè¿è¡Œ rfm_analysis.py")
        return
    
    rfm_df = pd.read_csv(rfm_path, index_col=0)
    
    # åˆ›å»ºé¢„æµ‹å™¨
    predictor = ChurnPredictor(rfm_df)
    
    # å‡†å¤‡æ•°æ®
    predictor.prepare_data(churn_threshold=60)
    
    # è®­ç»ƒæ¨¡å‹
    predictor.train_logistic_regression()
    
    # å¯¹æ¯”éšæœºæ£®æ—
    predictor.train_random_forest()
    
    # è¯„ä¼°
    predictor.evaluate_model()
    
    # ç”Ÿæˆå¯è§†åŒ–
    print("\n" + "="*60)
    print("ğŸ¨ ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    print("="*60)
    os.makedirs('../data', exist_ok=True)
    
    predictor.plot_confusion_matrix()
    predictor.plot_roc_curve()
    predictor.plot_feature_importance()
    
    # é¢„æµ‹æ‰€æœ‰ç”¨æˆ·
    predictor.predict_churn_risk()
    
    # ä¿å­˜æ¨¡å‹
    predictor.save_model()
    
    print("\n" + "="*60)
    print("âœ… æµå¤±é¢„æµ‹åˆ†æå®Œæˆ!")
    print("="*60)


if __name__ == '__main__':
    main()
